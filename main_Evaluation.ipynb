{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on the test(validation) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.miscellaneous import read_config\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils.train_val_test_dataset_import as tvt\n",
    "import utils.evaluation_matrix as em\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1 Parse configuration file + initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read config files\n",
    "cfg = read_config('./config.yaml')\n",
    "\n",
    "# constants\n",
    "num_classes = cfg['num_classes']\n",
    "image_height = cfg['image_height']\n",
    "image_width = cfg['image_width']\n",
    "batch_size = cfg['batch_size']['tra']\n",
    "\n",
    "labels = cfg['labels']\n",
    "\n",
    "# paths\n",
    "path_test = cfg['Path']['path_test']\n",
    "save_misclassified_images_path = cfg['Path']['save_misclassified_images_path']\n",
    "# model_path = cfg['Path']['model_path']\n",
    "\n",
    "# model_path = r\"F:\\Tianlong\\PythonProject\\deep_plastic\\output\\TL_vs_scratch\\model_trained_for_GradCam\\weights\\mobile_net_lr_0.0001_TL_all_GC.hdf5\"\n",
    "model_path = r\"F:\\Tianlong\\PythonProject\\deep_plastic\\output\\TL_vs_scratch\\from_scratch_weights\\mobile_net_lr_0.0001.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = tvt.import_dataset_test(\n",
    "    path_test, image_height, image_width, batch_size)\n",
    "\n",
    "# autotune\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "ds_test = ds_test.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the test DATA\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, label_s in ds_test.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1) \n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Statistics output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics output when evaluated on the test data\n",
    " \n",
    "loss, acc = model.evaluate(ds_test)\n",
    "print(\"Model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.cnn_statistics(model,ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.ConfusionMatrix('MobileNet', model, ds_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Show and save Misclassified examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.concatenate([y for x, y in ds_test], axis=0)\n",
    "y_pred = model.predict(ds_test).argmax(axis=1)\n",
    "x_test = np.concatenate([x for x, y in ds_test], axis=0)\n",
    "x_test = x_test/255.0\n",
    "misclassified_idx = np.where(y_pred != y_true)[0]\n",
    "print(\"Number of misclassified images: \", misclassified_idx.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misclassified_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save misclassified images in one folder\n",
    "\n",
    "for i in misclassified_idx:\n",
    "    fig = plt.figure(figsize=(2.24,2.24)) \n",
    "    plt.axis('off')\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_test[i], cmap='gray')\n",
    "    plt.title(\"%s pred as: %s\" % (labels[y_true[i]], labels[y_pred[i]]))\n",
    "    fig_name = 'misclass' + str(i) + \".jpg\"\n",
    "    fig_path = os.path.join(save_misclassified_images_path,fig_name)\n",
    "    fig.savefig(fig_path)\n",
    "    plt.close() # close the figure, to prevent the figure shows up in Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show one random misclassified example\n",
    "i = np.random.choice(misclassified_idx)\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.grid(False)\n",
    "plt.imshow(x_test[i], cmap='gray')\n",
    "plt.title(\"True label: %s, Predicted: %s\" % (labels[y_true[i]], labels[y_pred[i]]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show one specific misclassified example\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.grid(False)\n",
    "plt.imshow(x_test[0], cmap='gray')\n",
    "plt.title(\"True label: %s, Predicted: %s\" % (labels[y_true[160]], labels[y_pred[0]]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=4\n",
    "y_true = np.concatenate([y for x, y in ds_val], axis=0)\n",
    "y_pred_squeeze = model_squeeze.predict(ds_val).argmax(axis=1)\n",
    "y_pred_dense = model_dense.predict(ds_val).argmax(axis=1)\n",
    "y_pred_incept = model_incept.predict(ds_val).argmax(axis=1)\n",
    "major = [0]*len(y_pred_incept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'squeezenet': y_pred_squeeze, 'densenet121': y_pred_dense, 'inceptionv3': y_pred_incept, 'm_vote': major }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(data['squeezenet'])):\n",
    "    if (data['squeezenet'][i] != data['densenet121'][i]) and (data['squeezenet'][i] != data['inceptionv3'][i]) and (data['densenet121'][i] != data['inceptionv3'][i]):\n",
    "        df['m_vote'][i] = data['squeezenet'][i]\n",
    "    elif (data['squeezenet'][i] == data['densenet121'][i]):\n",
    "        df['m_vote'][i] = data['squeezenet'][i]\n",
    "    elif (data['squeezenet'][i] == data['inceptionv3'][i]):\n",
    "        df['m_vote'][i] = data['squeezenet'][i]\n",
    "    elif (data['densenet121'][i] == data['inceptionv3'][i]):\n",
    "        df['m_vote'][i] = data['densenet121'][i]\n",
    "    else:\n",
    "        df['m_vote'][i] = data['squeezenet'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(df['m_vote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(r'C:\\Users\\Andre\\Desktop\\Research paper\\from_scratch_weights_with_DA\\dense_net_DAHV.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(ds_val).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Print classification report on validation(test) dataset')\n",
    "print(f'-----'*10)\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "  # we quickly create a decent looking CM for many classes using pandas and seaborn\n",
    "print(f'\\nPlot confusion matrix on validation(test) dataset')\n",
    "print(f'-----'*10)\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_true,y_pred))\n",
    "f, ax = plt.subplots(1,figsize = (10,10))\n",
    "sns.heatmap(df_cm, annot=True, annot_kws={\"fontsize\":12}, ax= ax, vmax=50, cbar=False, cmap='Blues', fmt='d') # we use this value of vmax to emphasize misclassifications\n",
    "ax.set_title('SqueezeNet - TRAIN: Train 3, Test: Test 4', fontsize=16, fontweight='bold')\n",
    "ax.set_ylabel('True class');\n",
    "ax.set_xlabel('Predicted class');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority vote with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_squeeze = model_squeeze.predict(ds_val)\n",
    "y_pred_dense = model_dense.predict(ds_val)\n",
    "y_pred_incept = model_incept.predict(ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred_dense)):\n",
    "    for j in range(len(y_pred_dense[i])):\n",
    "        y_pred_dense[i][j] = (0.83*y_pred_incept[i][j]) + (0.86*y_pred_dense[i][j]) + (0.89*y_pred_squeeze[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred_dense, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Print classification report on validation(test) dataset')\n",
    "print(f'-----'*10)\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "\n",
    "print(f'\\nPlot confusion matrix on validation(test) dataset')\n",
    "print(f'-----'*10)\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_val,y_pred))\n",
    "f, ax = plt.subplots(1,figsize = (10,10))\n",
    "sns.heatmap(df_cm, annot=True, annot_kws={\"fontsize\":12}, ax= ax, vmax=50, cbar=False, cmap='Blues', fmt='d') # we use this value of vmax to emphasize misclassifications\n",
    "ax.set_title('Majority Vote - TRAIN: 2.7m/0 deg, VAL: 2.7m/0 deg', fontsize=16, fontweight='bold')\n",
    "ax.set_ylabel('True class');\n",
    "ax.set_xlabel('Predicted class');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('DP_tf_2.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "02a9bb87e7ed71d24a2ebc89c433b3cf6535eba82982e54a30fa825081488b7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
